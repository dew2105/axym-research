{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Ingestion Benchmark\n",
    "\n",
    "**Research Question:** How does ingesting 2.9 GB of real Medicaid claims data compare across a traditional multi-database stack (PostgreSQL + DuckDB + Neo4j) vs. AXYM's unified platform?\n",
    "\n",
    "## What We're Measuring\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| Wall clock time | End-to-end ingestion time per system |\n",
    "| CPU utilization | User + system CPU seconds consumed |\n",
    "| Peak memory | Maximum RSS during ingestion |\n",
    "| Disk footprint | Storage consumed by each database |\n",
    "| Operational complexity | Scripts, languages, containers, ETL pipelines required |\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**CMS Medicaid Provider Utilization & Spending** — aggregated provider-level claims from HHS/CMS.\n",
    "- ~89M rows of provider billing, procedure codes, beneficiary counts, and payment amounts\n",
    "- Source: Parquet format (2.9 GB)\n",
    "- Columns: `Billing_Provider_NPI`, `Servicing_Provider_NPI`, `HCPCS_Code`, `Claim_From_Month`, `Total_Unique_Beneficiaries`, `Total_Claims`, `Total_Paid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import platform\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config.settings import PARQUET_PATH, RESULTS_DIR\n",
    "from lib.metrics import BenchmarkResult\n",
    "from lib.report import (\n",
    "    comparison_table,\n",
    "    bar_chart_wall_time,\n",
    "    bar_chart_disk_footprint,\n",
    "    stacked_bar_traditional_vs_axym,\n",
    "    complexity_summary,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Platform:  {platform.platform()}\")\n",
    "print(f\"Python:    {platform.python_version()}\")\n",
    "print(f\"CPU:       {platform.processor()} ({psutil.cpu_count(logical=False)} cores, {psutil.cpu_count()} threads)\")\n",
    "print(f\"RAM:       {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"Disk free: {psutil.disk_usage('/').free / (1024**3):.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARQUET_PATH.exists():\n",
    "    pf = pq.ParquetFile(PARQUET_PATH)\n",
    "    print(f\"File:    {PARQUET_PATH.name}\")\n",
    "    print(f\"Size:    {PARQUET_PATH.stat().st_size / (1024**3):.2f} GB\")\n",
    "    print(f\"Rows:    {pf.metadata.num_rows:,}\")\n",
    "    print(f\"Columns: {pf.metadata.num_columns}\")\n",
    "    print(f\"Row groups: {pf.metadata.num_row_groups}\")\n",
    "    print()\n",
    "    print(\"Schema:\")\n",
    "    print(pf.schema_arrow)\n",
    "else:\n",
    "    print(f\"Parquet file not found at {PARQUET_PATH}\")\n",
    "    print(\"Run `make setup` to download the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARQUET_PATH.exists():\n",
    "    # Read a small sample for exploration\n",
    "    sample = pf.read_row_group(0).to_pandas().head(10)\n",
    "    display(sample)\n",
    "    print(f\"\\nDtypes:\")\n",
    "    print(sample.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARQUET_PATH.exists():\n",
    "    # Cardinality of key columns\n",
    "    sample_large = pf.read_row_group(0).to_pandas()\n",
    "    print(\"Cardinality (first row group):\")\n",
    "    for col in [\"Billing_Provider_NPI\", \"Servicing_Provider_NPI\", \"HCPCS_Code\"]:\n",
    "        print(f\"  {col}: {sample_large[col].nunique():,} unique values\")\n",
    "    print(f\"  Claim_From_Month range: {sample_large['Claim_From_Month'].min()} to {sample_large['Claim_From_Month'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traditional Stack — Individual Ingestions\n",
    "\n",
    "Results are loaded from JSON files produced by `make benchmark`. Each ingestion script measures wall time, CPU, memory, disk, and row count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_result(name: str) -> BenchmarkResult | None:\n",
    "    \"\"\"Load a benchmark result from JSON, or return None.\"\"\"\n",
    "    path = RESULTS_DIR / f\"ingest_{name}.json\"\n",
    "    if path.exists():\n",
    "        return BenchmarkResult.load(path)\n",
    "    print(f\"Result not found: {path}\")\n",
    "    print(\"Run `make benchmark` to generate results.\")\n",
    "    return None\n",
    "\n",
    "pg_result = load_result(\"postgres\")\n",
    "duck_result = load_result(\"duckdb\")\n",
    "neo4j_result = load_result(\"neo4j\")\n",
    "axym_result = load_result(\"axym\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. PostgreSQL\n",
    "\n",
    "Ingestion method: Read Parquet in batches via PyArrow → convert to CSV in memory → `COPY` protocol via psycopg3. Indexes on NPI, HCPCS, and date columns are built during ingestion (included in timing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pg_result:\n",
    "    print(f\"PostgreSQL Ingestion\")\n",
    "    print(f\"  Rows:      {pg_result.row_count:,}\")\n",
    "    print(f\"  Wall time: {pg_result.wall_time_seconds:.1f}s\")\n",
    "    print(f\"  CPU:       {pg_result.cpu_user_seconds:.1f}s user, {pg_result.cpu_system_seconds:.1f}s sys\")\n",
    "    print(f\"  Memory:    {pg_result.peak_memory_mb:,.0f} MB peak\")\n",
    "    print(f\"  Disk:      {pg_result.disk_mb:,.0f} MB\")\n",
    "    print(f\"  Throughput:{pg_result.rows_per_second:,.0f} rows/sec\")\n",
    "    if pg_result.error:\n",
    "        print(f\"  ERROR: {pg_result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. DuckDB\n",
    "\n",
    "Ingestion method: Single SQL statement — `CREATE TABLE AS SELECT * FROM read_parquet(...)`. DuckDB reads Parquet natively with zero ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if duck_result:\n",
    "    print(f\"DuckDB Ingestion\")\n",
    "    print(f\"  Rows:      {duck_result.row_count:,}\")\n",
    "    print(f\"  Wall time: {duck_result.wall_time_seconds:.1f}s\")\n",
    "    print(f\"  CPU:       {duck_result.cpu_user_seconds:.1f}s user, {duck_result.cpu_system_seconds:.1f}s sys\")\n",
    "    print(f\"  Memory:    {duck_result.peak_memory_mb:,.0f} MB peak\")\n",
    "    print(f\"  Disk:      {duck_result.disk_mb:,.0f} MB\")\n",
    "    print(f\"  Throughput:{duck_result.rows_per_second:,.0f} rows/sec\")\n",
    "    if duck_result.error:\n",
    "        print(f\"  ERROR: {duck_result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Neo4j\n",
    "\n",
    "Ingestion method: Multi-phase ETL pipeline:\n",
    "1. Extract distinct Provider nodes (union of billing + servicing NPIs)\n",
    "2. Extract distinct Procedure nodes (unique HCPCS codes)\n",
    "3. Build relationship CSVs (BILLED_FOR, REFERRED_TO edges)\n",
    "4. Copy CSVs to Neo4j import volume\n",
    "5. `LOAD CSV` with periodic commits\n",
    "\n",
    "The ETL overhead — transforming tabular data into graph structure — is a key finding about impedance mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if neo4j_result:\n",
    "    print(f\"Neo4j Ingestion\")\n",
    "    print(f\"  Nodes+Rels: {neo4j_result.row_count:,}\")\n",
    "    print(f\"  Wall time:  {neo4j_result.wall_time_seconds:.1f}s\")\n",
    "    print(f\"  CPU:        {neo4j_result.cpu_user_seconds:.1f}s user, {neo4j_result.cpu_system_seconds:.1f}s sys\")\n",
    "    print(f\"  Memory:     {neo4j_result.peak_memory_mb:,.0f} MB peak\")\n",
    "    print(f\"  Disk:       {neo4j_result.disk_mb:,.0f} MB\")\n",
    "    if neo4j_result.error:\n",
    "        print(f\"  ERROR: {neo4j_result.error}\")\n",
    "    \n",
    "    if neo4j_result.metadata.get(\"timings\"):\n",
    "        print(f\"\\n  ETL Phase Breakdown:\")\n",
    "        for phase, secs in neo4j_result.metadata[\"timings\"].items():\n",
    "            print(f\"    {phase}: {secs:.1f}s\")\n",
    "    \n",
    "    if neo4j_result.metadata.get(\"counts\"):\n",
    "        print(f\"\\n  Graph Counts:\")\n",
    "        for k, v in neo4j_result.metadata[\"counts\"].items():\n",
    "            print(f\"    {k}: {v:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traditional Stack — Aggregate\n",
    "\n",
    "To run the \"traditional\" Medicaid fraud detection pipeline, you need **all three** databases. The combined cost is the sum of individual ingestion times, disk footprints, and operational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traditional = [r for r in [pg_result, duck_result, neo4j_result] if r and not r.error]\n",
    "\n",
    "if traditional:\n",
    "    total_wall = sum(r.wall_time_seconds for r in traditional)\n",
    "    total_disk = sum(r.disk_mb for r in traditional)\n",
    "    print(f\"Traditional Stack Total:\")\n",
    "    print(f\"  Combined wall time: {total_wall:.1f}s\")\n",
    "    print(f\"  Combined disk:      {total_disk:,.0f} MB\")\n",
    "    print(f\"  Systems:            {len(traditional)}\")\n",
    "    print(f\"  Scripts required:   3 (each with different driver/protocol)\")\n",
    "    print(f\"  Query languages:    2 (SQL + Cypher)\")\n",
    "    print(f\"  Docker containers:  2 (PostgreSQL + Neo4j)\")\n",
    "    print(f\"  ETL pipelines:      1 (tabular → graph for Neo4j)\")\n",
    "else:\n",
    "    print(\"No results available. Run `make benchmark` first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AXYM\n",
    "\n",
    "AXYM aims to replace this entire stack with a single unified platform. One ingestion command, one query language, zero ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if axym_result and not axym_result.error:\n",
    "    print(f\"AXYM Ingestion\")\n",
    "    print(f\"  Rows:      {axym_result.row_count:,}\")\n",
    "    print(f\"  Wall time: {axym_result.wall_time_seconds:.1f}s\")\n",
    "    print(f\"  Disk:      {axym_result.disk_mb:,.0f} MB\")\n",
    "    print(f\"  Throughput:{axym_result.rows_per_second:,.0f} rows/sec\")\n",
    "else:\n",
    "    print(\"AXYM CLI is not yet available.\")\n",
    "    print(\"Results will be added once the CLI ships.\")\n",
    "    print(\"\")\n",
    "    print(\"Expected interface:\")\n",
    "    print(\"  axym ingest medicaid-provider-spending.parquet\")\n",
    "    print(\"  → Automatically creates tables, indexes, graph structure, and embeddings\")\n",
    "    print(\"  → Single command, zero ETL, zero configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = [r for r in [pg_result, duck_result, neo4j_result] if r and not r.error]\n",
    "\n",
    "if all_results:\n",
    "    display(comparison_table(all_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    bar_chart_wall_time(all_results)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    bar_chart_disk_footprint(all_results)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if traditional:\n",
    "    stacked_bar_traditional_vs_axym(traditional, axym_result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if traditional:\n",
    "    display(complexity_summary(traditional))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings\n",
    "\n",
    "### What the numbers show\n",
    "\n",
    "1. **DuckDB is fastest for pure tabular ingestion** — native Parquet support means zero conversion overhead. A single SQL statement loads the entire dataset.\n",
    "\n",
    "2. **PostgreSQL adds conversion overhead** — Parquet → CSV-in-memory → COPY protocol. This is the realistic scenario: PostgreSQL doesn't read Parquet natively, so an ETL step is unavoidable.\n",
    "\n",
    "3. **Neo4j has the highest cost** — not because the database is slow, but because tabular data must be **transformed** into a graph structure before loading. The ETL phases (extract nodes, build edges, write CSVs) add significant wall time.\n",
    "\n",
    "4. **The traditional stack requires all three** — for a fraud detection pipeline you need relational queries (PostgreSQL), analytical queries (DuckDB), and graph traversal (Neo4j). The combined cost is the sum.\n",
    "\n",
    "### Implications for Steps 2–6\n",
    "\n",
    "- **Step 2 (Querying)**: Three different query languages and APIs to maintain\n",
    "- **Step 3 (Graph Traversal)**: Only Neo4j handles this — the ETL tax is paid at ingestion\n",
    "- **Step 4 (Embeddings)**: Requires yet another system (pgvector or standalone)\n",
    "- **Step 5 (RAG)**: Orchestrating across all systems for a single query\n",
    "- **Step 6 (Full Pipeline)**: The operational complexity multiplies at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reproducibility\n",
    "\n",
    "### Data\n",
    "- Source: CMS Medicaid Provider Utilization & Spending\n",
    "- Format: Parquet (2.9 GB)\n",
    "- SHA256: `a998e5ae11a391f1eb0d8464b3866a3ee7fe18aa13e56d411c50e72e3a0e35c7`\n",
    "\n",
    "### Reproduce\n",
    "```bash\n",
    "git clone <repo-url> axym-research && cd axym-research\n",
    "python -m venv .venv && source .venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "make setup      # ~10 min (2.9 GB download)\n",
    "make benchmark  # ~30-60 min depending on hardware\n",
    "make notebook   # View results\n",
    "```\n",
    "\n",
    "### Methodology\n",
    "- Cold start: tables truncated / databases deleted between runs\n",
    "- Docker memory limits: 4 GB per container\n",
    "- Ingestion timing includes index creation (fair comparison)\n",
    "- Neo4j ETL phases timed individually\n",
    "- Results persisted as JSON in `results/` for inspection without re-running"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
