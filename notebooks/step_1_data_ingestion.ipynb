{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 1: Data Ingestion Benchmark\n\n**Research Question:** How does ingesting 2.9 GB of real Medicaid claims data compare across a traditional multi-database stack (PostgreSQL + DuckDB + PostgreSQL Graph Tables) vs. ΛXYM's unified platform?\n\n## What We're Measuring\n\n| Metric | Description |\n|--------|-------------|\n| Wall clock time | End-to-end ingestion time per system |\n| CPU utilization | User + system CPU seconds consumed |\n| Peak memory | Maximum RSS during ingestion |\n| Disk footprint | Storage consumed by each database |\n| Operational complexity | Scripts, languages, containers, ETL pipelines required |\n\n## Dataset\n\n**CMS Medicaid Provider Utilization & Spending** — aggregated provider-level claims from HHS/CMS.\n- ~89M rows of provider billing, procedure codes, beneficiary counts, and payment amounts\n- Source: Parquet format (2.9 GB)\n- Columns: `Billing_Provider_NPI`, `Servicing_Provider_NPI`, `HCPCS_Code`, `Claim_From_Month`, `Total_Unique_Beneficiaries`, `Total_Claims`, `Total_Paid`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import platform\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config.settings import PARQUET_PATH, RESULTS_DIR\n",
    "from lib.metrics import BenchmarkResult\n",
    "from lib.report import (\n",
    "    comparison_table,\n",
    "    bar_chart_wall_time,\n",
    "    bar_chart_disk_footprint,\n",
    "    stacked_bar_traditional_vs_axym,\n",
    "    complexity_summary,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Platform:  {platform.platform()}\")\n",
    "print(f\"Python:    {platform.python_version()}\")\n",
    "print(f\"CPU:       {platform.processor()} ({psutil.cpu_count(logical=False)} cores, {psutil.cpu_count()} threads)\")\n",
    "print(f\"RAM:       {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"Disk free: {psutil.disk_usage('/').free / (1024**3):.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARQUET_PATH.exists():\n",
    "    pf = pq.ParquetFile(PARQUET_PATH)\n",
    "    print(f\"File:    {PARQUET_PATH.name}\")\n",
    "    print(f\"Size:    {PARQUET_PATH.stat().st_size / (1024**3):.2f} GB\")\n",
    "    print(f\"Rows:    {pf.metadata.num_rows:,}\")\n",
    "    print(f\"Columns: {pf.metadata.num_columns}\")\n",
    "    print(f\"Row groups: {pf.metadata.num_row_groups}\")\n",
    "    print()\n",
    "    print(\"Schema:\")\n",
    "    print(pf.schema_arrow)\n",
    "else:\n",
    "    print(f\"Parquet file not found at {PARQUET_PATH}\")\n",
    "    print(\"Run `make setup` to download the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARQUET_PATH.exists():\n",
    "    # Read a small sample for exploration\n",
    "    sample = pf.read_row_group(0).to_pandas().head(10)\n",
    "    display(sample)\n",
    "    print(f\"\\nDtypes:\")\n",
    "    print(sample.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARQUET_PATH.exists():\n",
    "    # Cardinality of key columns\n",
    "    sample_large = pf.read_row_group(0).to_pandas()\n",
    "    print(\"Cardinality (first row group):\")\n",
    "    for col in [\"Billing_Provider_NPI\", \"Servicing_Provider_NPI\", \"HCPCS_Code\"]:\n",
    "        print(f\"  {col}: {sample_large[col].nunique():,} unique values\")\n",
    "    print(f\"  Claim_From_Month range: {sample_large['Claim_From_Month'].min()} to {sample_large['Claim_From_Month'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traditional Stack — Individual Ingestions\n",
    "\n",
    "Results are loaded from JSON files produced by `make benchmark`. Each ingestion script measures wall time, CPU, memory, disk, and row count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_result(name: str) -> BenchmarkResult | None:\n    \"\"\"Load a benchmark result from JSON, or return None.\"\"\"\n    path = RESULTS_DIR / f\"ingest_{name}.json\"\n    if path.exists():\n        return BenchmarkResult.load(path)\n    print(f\"Result not found: {path}\")\n    print(\"Run `make benchmark` to generate results.\")\n    return None\n\npg_result = load_result(\"postgres\")\nduck_result = load_result(\"duckdb\")\ngraph_result = load_result(\"graph\")\naxym_result = load_result(\"axym\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. PostgreSQL\n",
    "\n",
    "Ingestion method: Read Parquet in batches via PyArrow → convert to CSV in memory → `COPY` protocol via psycopg3. Indexes on NPI, HCPCS, and date columns are built during ingestion (included in timing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pg_result:\n",
    "    print(f\"PostgreSQL Ingestion\")\n",
    "    print(f\"  Rows:      {pg_result.row_count:,}\")\n",
    "    print(f\"  Wall time: {pg_result.wall_time_seconds:.1f}s\")\n",
    "    print(f\"  CPU:       {pg_result.cpu_user_seconds:.1f}s user, {pg_result.cpu_system_seconds:.1f}s sys\")\n",
    "    print(f\"  Memory:    {pg_result.peak_memory_mb:,.0f} MB peak\")\n",
    "    print(f\"  Disk:      {pg_result.disk_mb:,.0f} MB\")\n",
    "    print(f\"  Throughput:{pg_result.rows_per_second:,.0f} rows/sec\")\n",
    "    if pg_result.error:\n",
    "        print(f\"  ERROR: {pg_result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. DuckDB\n",
    "\n",
    "Ingestion method: Single SQL statement — `CREATE TABLE AS SELECT * FROM read_parquet(...)`. DuckDB reads Parquet natively with zero ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if duck_result:\n",
    "    print(f\"DuckDB Ingestion\")\n",
    "    print(f\"  Rows:      {duck_result.row_count:,}\")\n",
    "    print(f\"  Wall time: {duck_result.wall_time_seconds:.1f}s\")\n",
    "    print(f\"  CPU:       {duck_result.cpu_user_seconds:.1f}s user, {duck_result.cpu_system_seconds:.1f}s sys\")\n",
    "    print(f\"  Memory:    {duck_result.peak_memory_mb:,.0f} MB peak\")\n",
    "    print(f\"  Disk:      {duck_result.disk_mb:,.0f} MB\")\n",
    "    print(f\"  Throughput:{duck_result.rows_per_second:,.0f} rows/sec\")\n",
    "    if duck_result.error:\n",
    "        print(f\"  ERROR: {duck_result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3c. Graph (PostgreSQL)\n\nIngestion method: Multi-phase ETL pipeline that builds explicit graph tables in PostgreSQL (same Neon instance):\n\n1. Create node tables: `graph_providers` (unique NPIs) and `graph_procedures` (unique HCPCS codes)\n2. Create edge tables: `graph_billed_for` and `graph_referred_to`\n3. Populate all tables via `INSERT INTO ... SELECT` from `medicaid_claims` (server-side, no data transfer to client)\n4. Build indexes for graph traversal patterns\n5. `ANALYZE` all graph tables\n\n**Full dataset** — no sampling. All 227M+ rows are used, providing a fair comparison against PostgreSQL and DuckDB.\n\n**Why not a dedicated graph database?** Neo4j AuraDB and TigerGraph Savanna were evaluated and rejected due to cost:\n\n| Platform | Tier | Cost | Issue |\n|----------|------|------|-------|\n| Neo4j AuraDB Free | 200K nodes / 400K rels | $0 | Requires 99.8% sampling (dataset has 1.81M nodes, 227M+ rels) |\n| Neo4j AuraDB Pro | 64 GB | $5.76/hr ($4,160/mo) | Too expensive for research benchmark |\n| TigerGraph Savanna | TG-1 (64 GB) | $4.00/hr ($2,920/mo) | No persistent free tier |\n| PostgreSQL (Neon) | Already provisioned | $0 incremental | Selected — demonstrates ETL overhead |\n\nThe ETL overhead — transforming tabular data into explicit graph structure — is a key finding about impedance mismatch. Graph queries use recursive CTEs instead of Cypher."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if graph_result:\n    print(f\"Graph (PostgreSQL) Ingestion\")\n    print(f\"  Nodes+Rels: {graph_result.row_count:,}\")\n    print(f\"  Wall time:  {graph_result.wall_time_seconds:.1f}s\")\n    print(f\"  CPU:        {graph_result.cpu_user_seconds:.1f}s user, {graph_result.cpu_system_seconds:.1f}s sys\")\n    print(f\"  Memory:     {graph_result.peak_memory_mb:,.0f} MB peak\")\n    print(f\"  Disk:       {graph_result.disk_mb:,.0f} MB\")\n    if graph_result.error:\n        print(f\"  ERROR: {graph_result.error}\")\n    \n    if graph_result.metadata.get(\"timings\"):\n        print(f\"\\n  ETL Phase Breakdown:\")\n        for phase, secs in graph_result.metadata[\"timings\"].items():\n            print(f\"    {phase}: {secs:.1f}s\")\n    \n    if graph_result.metadata.get(\"counts\"):\n        print(f\"\\n  Graph Counts:\")\n        for k, v in graph_result.metadata[\"counts\"].items():\n            print(f\"    {k}: {v:,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traditional Stack — Aggregate\n",
    "\n",
    "To run the \"traditional\" Medicaid fraud detection pipeline, you need **all three** databases. The combined cost is the sum of individual ingestion times, disk footprints, and operational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "traditional = [r for r in [pg_result, duck_result, graph_result] if r and not r.error]\n\nif traditional:\n    total_wall = sum(r.wall_time_seconds for r in traditional)\n    total_disk = sum(r.disk_mb for r in traditional)\n    print(f\"Traditional Stack Total:\")\n    print(f\"  Combined wall time: {total_wall:.1f}s\")\n    print(f\"  Combined disk:      {total_disk:,.0f} MB\")\n    print(f\"  Systems:            {len(traditional)}\")\n    print(f\"  Scripts required:   3 (each with different driver/protocol)\")\n    print(f\"  Query languages:    1 (SQL + recursive CTEs)\")\n    print(f\"  Docker containers:  0 (all hosted)\")\n    print(f\"  ETL pipelines:      1 (tabular → graph for PostgreSQL graph tables)\")\nelse:\n    print(\"No results available. Run `make benchmark` first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. ΛXYM\n\nΛXYM aims to replace this entire stack with a single unified platform. One ingestion command, one query language, zero ETL."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if axym_result and not axym_result.error:\n    print(f\"ΛXYM Ingestion\")\n    print(f\"  Rows:      {axym_result.row_count:,}\")\n    print(f\"  Wall time: {axym_result.wall_time_seconds:.1f}s\")\n    print(f\"  Disk:      {axym_result.disk_mb:,.0f} MB\")\n    print(f\"  Throughput:{axym_result.rows_per_second:,.0f} rows/sec\")\nelse:\n    print(\"ΛXYM CLI is not yet available.\")\n    print(\"Results will be added once the CLI ships.\")\n    print(\"\")\n    print(\"Expected interface:\")\n    print(\"  axym ingest medicaid-provider-spending.parquet\")\n    print(\"  → Automatically creates tables, indexes, graph structure, and embeddings\")\n    print(\"  → Single command, zero ETL, zero configuration\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "all_results = [r for r in [pg_result, duck_result, graph_result] if r and not r.error]\n\nif all_results:\n    display(comparison_table(all_results))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    bar_chart_wall_time(all_results)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    bar_chart_disk_footprint(all_results)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if traditional:\n",
    "    stacked_bar_traditional_vs_axym(traditional, axym_result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if traditional:\n",
    "    display(complexity_summary(traditional))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Key Findings\n\n### What the numbers show\n\n1. **DuckDB is fastest for pure tabular ingestion** — native Parquet support means zero conversion overhead. A single SQL statement loads the entire dataset.\n\n2. **PostgreSQL adds conversion overhead** — Parquet → CSV-in-memory → COPY protocol. This is the realistic scenario: PostgreSQL doesn't read Parquet natively, so an ETL step is unavoidable.\n\n3. **Graph table ETL has the highest overhead** — not because PostgreSQL is slow, but because tabular data must be **transformed** into explicit graph structure (node/edge tables) before it can support graph queries. The `INSERT...SELECT` phases (extract distinct providers, procedures, build edge tables) add significant wall time. Unlike the original Neo4j approach, this uses the **full dataset** with no sampling.\n\n4. **The traditional stack requires all three ingestion paths** — for a fraud detection pipeline you need relational queries (PostgreSQL), analytical queries (DuckDB), and graph traversal (graph tables + recursive CTEs). The combined cost is the sum.\n\n5. **Dedicated graph databases are prohibitively expensive** — Neo4j AuraDB Pro (~$5.76/hr for 64 GB) and TigerGraph Savanna (~$4/hr for 64 GB) were evaluated and rejected. The PostgreSQL graph table approach costs $0 incremental but demonstrates the impedance mismatch.\n\n### Implications for Steps 2–6\n\n- **Step 2 (Querying)**: SQL across all systems, but graph queries require verbose recursive CTEs\n- **Step 3 (Graph Traversal)**: Only graph tables handle this — the ETL tax is paid at ingestion\n- **Step 4 (Embeddings)**: Requires yet another system (pgvector or standalone)\n- **Step 5 (RAG)**: Orchestrating across all systems for a single query\n- **Step 6 (Full Pipeline)**: The operational complexity multiplies at each step"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Reproducibility\n\n### Data\n- Source: CMS Medicaid Provider Utilization & Spending\n- Format: Parquet (2.9 GB)\n- SHA256: `a998e5ae11a391f1eb0d8464b3866a3ee7fe18aa13e56d411c50e72e3a0e35c7`\n\n### Reproduce\n```bash\ngit clone <repo-url> axym-research && cd axym-research\npython -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\n\n# Configure cloud credentials\ncp .env.example .env\n# Edit .env with your MotherDuck token and Neon DSN\n\nmake setup      # Verify connections + download data (~10 min)\nmake benchmark  # Run all ingestion benchmarks (~30-60 min)\nmake notebook   # View results\n```\n\n### Methodology\n- Cold start: tables truncated / databases cleared between runs\n- All databases hosted (MotherDuck, Neon — graph tables reuse the same Neon instance)\n- Ingestion timing includes index creation (fair comparison)\n- Graph ETL phases timed individually; full dataset used (no sampling)\n- Dedicated graph DBs (Neo4j AuraDB, TigerGraph Savanna) evaluated and rejected due to cost — see `docs/graph_database_cost_analysis.md`\n- Results persisted as JSON in `results/` for inspection without re-running"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}